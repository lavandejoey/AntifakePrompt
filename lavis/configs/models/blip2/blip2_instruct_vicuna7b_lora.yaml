 # Copyright (c) 2022, salesforce.com, inc.
 # All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause

model:
  arch: blip2_vicuna_instruct_lora
  load_finetuned: True
  load_pretrained: True

  pretrained: "https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/InstructBLIP/instruct_blip_vicuna7b_trimmed.pth"
  # finetuned: "/home/denny/LAVIS/lavis/output/BLIP2/Textual_inv/20230925210/checkpoint_best.pth" # SD2+IP (90k, lora)
  # finetuned: "/home/denny/LAVIS/lavis/output/BLIP2/Textual_inv/20230925210/checkpoint_best.pth" # SD2+IP (90k, lora on Q)
  finetuned: "/home/denny/LAVIS/lavis/output/BLIP2/Textual_inv/20230926184/checkpoint_best.pth" # SD2+IP (90k, lora on LLM)
  
  # vit encoder
  image_size: 224
  drop_path_rate: 0
  use_grad_checkpoint: False
  vit_precision: "fp16"
  freeze_vit: True

  # Q-Former
  num_query_token: 32

  # path to Vicuna checkpoint
  llm_model: "lmsys/vicuna-7b-v1.5"

  # generation configs
  prompt: "Is this photo real?"
  pseudo_word: "[*]"
  # init_word: "real"

  # lora
  lora_on_Qformer: False
  lora_on_LLM: True
  lora_inference_mode: True


preprocess:
    vis_processor:
        train:
          name: "blip2_image_train_textinv"
          image_size: 224
          # # COCO (30k) + SD2 + SD2IP 
          # mean: [0.4750, 0.4520, 0.4165]
          # std: [0.2778, 0.2697, 0.2864]
          # # COCO (60k) + SD2 + SD2IP 
          # mean: [0.4739, 0.4508, 0.4143]
          # std: [0.2778, 0.2706, 0.2868]
          # COCO (90k) + SD2 + SD2IP 
          mean: [0.4730, 0.4499, 0.4129]
          std: [0.2780, 0.2713, 0.2872]
          # # COCO (120k) + SD2 + SD2IP 
          # mean: [0.4723, 0.4492, 0.4119]
          # std: [0.2781, 0.2717, 0.2874]
          # # COCO (9k) + SD2 + SD2IP 
          # mean: [0.4728, 0.4518, 0.4146]
          # std: [0.2778, 0.2709, 0.2870]
          # # COCO (900) + SD2 + SD2IP 
          # mean: [0.4711, 0.4528, 0.4147]
          # std: [0.2756, 0.2690, 0.2872]
          # # COCO (90) + SD2 + SD2IP 
          # mean: [0.4860, 0.4659, 0.4203]
          # std: [0.2713, 0.2658, 0.2806]
        eval:
          name: "blip_image_eval_textinv"
          image_size: 224
          # # COCO (30k) + SD2 + SD2IP 
          # mean: [0.4750, 0.4520, 0.4165]
          # std: [0.2778, 0.2697, 0.2864]
          # # COCO (60k) + SD2 + SD2IP 
          # mean: [0.4739, 0.4508, 0.4143]
          # std: [0.2778, 0.2706, 0.2868]
          # COCO (90k) + SD2 + SD2IP 
          mean: [0.4730, 0.4499, 0.4129]
          std: [0.2780, 0.2713, 0.2872]
          # # COCO (120k) + SD2 + SD2IP 
          # mean: [0.4723, 0.4492, 0.4119]
          # std: [0.2781, 0.2717, 0.2874]
          # # COCO (9k) + SD2 + SD2IP 
          # mean: [0.4728, 0.4518, 0.4146]
          # std: [0.2778, 0.2709, 0.2870]
          # # COCO (900) + SD2 + SD2IP 
          # mean: [0.4711, 0.4528, 0.4147]
          # std: [0.2756, 0.2690, 0.2872]
          # # COCO (90) + SD2 + SD2IP 
          # mean: [0.4860, 0.4659, 0.4203]
          # std: [0.2713, 0.2658, 0.2806]
    text_processor:
        train:
          name: "blip_question_textinv"
          prompt: "Is this photo real?"
          pseudo_word: "[*]"
        eval:
          name: "blip_question_textinv"
          prompt: "Is this photo real?"
          pseudo_word: "[*]"
